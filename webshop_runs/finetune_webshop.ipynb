{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bdaf1346-cff8-4c48-94be-e4e8649e9d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import ast\n",
    "import json\n",
    "import pathlib\n",
    "import glob\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "# trial = \"state_dev\"\n",
    "# trial = \"state_train_all\"\n",
    "\n",
    "# trial = \"stateact-no-thoughts_train_all\"\n",
    "# trial = \"state-no-thoughts_rag_train_all\"\n",
    "\n",
    "# trial = \"react_train\"\n",
    "\n",
    "\n",
    "# trial = \"state_rag_selfgenerated_train2/\"\n",
    "trial = \"react_rag_selfgenerated_train2/\"\n",
    "\n",
    "# Configuration: adjust these paths as needed\n",
    "\n",
    "METADATA_CSV = f\"game_logs/{trial}/webshop_scores.csv\"\n",
    "METADATA_DIR = f\"game_logs/{trial}_metadata/\"\n",
    "PROMPT_BASE_DIR = f\"game_logs/{trial}/\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # Something to include to fix unsloth on jupyter\n",
    "os.environ[\"TRITON_DISABLE\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "27a50341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the metadata csv\n",
    "metadata = pd.read_csv(\"/workspace/data/webshop/WebShop_metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c0affac5-d569-40d7-af04-797a8e19c176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>category</th>\n",
       "      <th>product_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>find me sulfate free, paraben free shampoo &amp; c...</td>\n",
       "      <td>beauty</td>\n",
       "      <td>Beauty &amp; Personal Care › Hair Care › Shampoo &amp;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>find me non slip shave &amp; hair removal with sta...</td>\n",
       "      <td>beauty</td>\n",
       "      <td>Beauty &amp; Personal Care › Shave &amp; Hair Removal ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>find me women's dresses with polyester spandex...</td>\n",
       "      <td>fashion</td>\n",
       "      <td>Clothing, Shoes &amp; Jewelry › Women › Clothing ›...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>find me space saving, easy assemble living roo...</td>\n",
       "      <td>garden</td>\n",
       "      <td>Home &amp; Kitchen › Furniture › Dining Room Furni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>find me long lasting, easy apply, cruelty free...</td>\n",
       "      <td>beauty</td>\n",
       "      <td>Beauty &amp; Personal Care › Makeup › Face › Blush</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529102</th>\n",
       "      <td>find me teeth whitening toothpaste for sensiti...</td>\n",
       "      <td>beauty</td>\n",
       "      <td>Beauty &amp; Personal Care › Oral Care › Toothpaste</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529103</th>\n",
       "      <td>find me machine wash men's tank tops with poly...</td>\n",
       "      <td>fashion</td>\n",
       "      <td>Clothing, Shoes &amp; Jewelry › Novelty &amp; More › C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529104</th>\n",
       "      <td>find me high quality makeup brushes &amp; tools fo...</td>\n",
       "      <td>beauty</td>\n",
       "      <td>Beauty &amp; Personal Care › Tools &amp; Accessories ›...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529105</th>\n",
       "      <td>find me super soft throw blankets with printin...</td>\n",
       "      <td>garden</td>\n",
       "      <td>Home &amp; Kitchen › Bedding › Blankets &amp; Throws ›...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529106</th>\n",
       "      <td>find me easy carry, easy clean salon &amp; spa equ...</td>\n",
       "      <td>beauty</td>\n",
       "      <td>Beauty &amp; Personal Care › Salon &amp; Spa Equipment...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>529107 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              instruction category  \\\n",
       "0       find me sulfate free, paraben free shampoo & c...   beauty   \n",
       "1       find me non slip shave & hair removal with sta...   beauty   \n",
       "2       find me women's dresses with polyester spandex...  fashion   \n",
       "3       find me space saving, easy assemble living roo...   garden   \n",
       "4       find me long lasting, easy apply, cruelty free...   beauty   \n",
       "...                                                   ...      ...   \n",
       "529102  find me teeth whitening toothpaste for sensiti...   beauty   \n",
       "529103  find me machine wash men's tank tops with poly...  fashion   \n",
       "529104  find me high quality makeup brushes & tools fo...   beauty   \n",
       "529105  find me super soft throw blankets with printin...   garden   \n",
       "529106  find me easy carry, easy clean salon & spa equ...   beauty   \n",
       "\n",
       "                                         product_category  \n",
       "0       Beauty & Personal Care › Hair Care › Shampoo &...  \n",
       "1       Beauty & Personal Care › Shave & Hair Removal ...  \n",
       "2       Clothing, Shoes & Jewelry › Women › Clothing ›...  \n",
       "3       Home & Kitchen › Furniture › Dining Room Furni...  \n",
       "4          Beauty & Personal Care › Makeup › Face › Blush  \n",
       "...                                                   ...  \n",
       "529102    Beauty & Personal Care › Oral Care › Toothpaste  \n",
       "529103  Clothing, Shoes & Jewelry › Novelty & More › C...  \n",
       "529104  Beauty & Personal Care › Tools & Accessories ›...  \n",
       "529105  Home & Kitchen › Bedding › Blankets & Throws ›...  \n",
       "529106  Beauty & Personal Care › Salon & Spa Equipment...  \n",
       "\n",
       "[529107 rows x 3 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1725a5f6-1052-443c-8d9f-a0f3bcf550a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_expert = pd.read_csv(\"/workspace/data/webshop/webshop_human_index.csv\")\n",
    "\n",
    "df_train = pd.read_csv(METADATA_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "59285198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the eto dataset containing gold GPT trajectories\n",
    "with open(\"/workspace/data/webshop/webshop_sft.json\", \"r\") as f:\n",
    "    eto = json.load(f)\n",
    "    \n",
    "instructions = []\n",
    "categories = []\n",
    "for row in eto:\n",
    "    instr = row[\"conversations\"][2][\"value\"][33:-13]\n",
    "    instructions.append(instr)\n",
    "    # break\n",
    "\n",
    "df_eto = pd.DataFrame(eto)\n",
    "df_eto[\"instruction\"] = instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "13d30983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>reward</th>\n",
       "      <th>file</th>\n",
       "      <th>category</th>\n",
       "      <th>product_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i'm looking for a 4-tier shelving unit and tv ...</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>{'goal': {'asin': 'B09N382RP7', 'category': 'g...</td>\n",
       "      <td>garden</td>\n",
       "      <td>Home &amp; Kitchen › Furniture › Living Room Furni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i'm looking for 1.5 feet (10 pack) high speed ...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>{'goal': {'asin': 'B01HJWDQWA', 'category': 'e...</td>\n",
       "      <td>electronics</td>\n",
       "      <td>Electronics › Accessories &amp; Supplies › Audio &amp;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i would like to buy a heavy duty with a rocket...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>{'goal': {'asin': 'B08TVG2339', 'category': 'g...</td>\n",
       "      <td>garden</td>\n",
       "      <td>Tools &amp; Home Improvement › Electrical › Wall P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i want to find a tv stand made of solid wood f...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>{'goal': {'asin': 'B09LTRP7HW', 'category': 'g...</td>\n",
       "      <td>garden</td>\n",
       "      <td>Home &amp; Kitchen › Furniture › Living Room Furni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i need some skin care tools for dark circles w...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>{'goal': {'asin': 'B09FXG949K', 'category': 'b...</td>\n",
       "      <td>beauty</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1638</th>\n",
       "      <td>i need birthday candles for my birthday cake, ...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>{'goal': {'asin': 'B07RVDW2Z2', 'category': 'g...</td>\n",
       "      <td>grocery</td>\n",
       "      <td>Home &amp; Kitchen › Home Décor Products › Candles...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1639</th>\n",
       "      <td>i am looking for a teal color stainlesss steel...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>{'goal': {'asin': 'B092J891H7', 'category': 'e...</td>\n",
       "      <td>electronics</td>\n",
       "      <td>Electronics › Wearable Technology › Arm &amp; Wris...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1640</th>\n",
       "      <td>i need a pair of pink loafers for teen girls. ...</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>{'goal': {'asin': 'B096W49HCJ', 'category': 'f...</td>\n",
       "      <td>fashion</td>\n",
       "      <td>Clothing, Shoes &amp; Jewelry › Women › Shoes › Lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1641</th>\n",
       "      <td>i am looking for a hair salon capacity spray b...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>{'goal': {'asin': 'B08GCNBFX1', 'category': 'b...</td>\n",
       "      <td>beauty</td>\n",
       "      <td>Beauty &amp; Personal Care › Hair Care › Hair Colo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1642</th>\n",
       "      <td>i'm looking for living room furniture and kitc...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>{'goal': {'asin': 'B097MRL84T', 'category': 'g...</td>\n",
       "      <td>garden</td>\n",
       "      <td>Home &amp; Kitchen › Furniture › Living Room Furni...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1643 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            instruction    reward  \\\n",
       "0     i'm looking for a 4-tier shelving unit and tv ...  0.600000   \n",
       "1     i'm looking for 1.5 feet (10 pack) high speed ...  1.000000   \n",
       "2     i would like to buy a heavy duty with a rocket...  1.000000   \n",
       "3     i want to find a tv stand made of solid wood f...  0.666667   \n",
       "4     i need some skin care tools for dark circles w...  1.000000   \n",
       "...                                                 ...       ...   \n",
       "1638  i need birthday candles for my birthday cake, ...  0.500000   \n",
       "1639  i am looking for a teal color stainlesss steel...  1.000000   \n",
       "1640  i need a pair of pink loafers for teen girls. ...  0.750000   \n",
       "1641  i am looking for a hair salon capacity spray b...  0.500000   \n",
       "1642  i'm looking for living room furniture and kitc...  0.500000   \n",
       "\n",
       "                                                   file     category  \\\n",
       "0     {'goal': {'asin': 'B09N382RP7', 'category': 'g...       garden   \n",
       "1     {'goal': {'asin': 'B01HJWDQWA', 'category': 'e...  electronics   \n",
       "2     {'goal': {'asin': 'B08TVG2339', 'category': 'g...       garden   \n",
       "3     {'goal': {'asin': 'B09LTRP7HW', 'category': 'g...       garden   \n",
       "4     {'goal': {'asin': 'B09FXG949K', 'category': 'b...       beauty   \n",
       "...                                                 ...          ...   \n",
       "1638  {'goal': {'asin': 'B07RVDW2Z2', 'category': 'g...      grocery   \n",
       "1639  {'goal': {'asin': 'B092J891H7', 'category': 'e...  electronics   \n",
       "1640  {'goal': {'asin': 'B096W49HCJ', 'category': 'f...      fashion   \n",
       "1641  {'goal': {'asin': 'B08GCNBFX1', 'category': 'b...       beauty   \n",
       "1642  {'goal': {'asin': 'B097MRL84T', 'category': 'g...       garden   \n",
       "\n",
       "                                       product_category  \n",
       "0     Home & Kitchen › Furniture › Living Room Furni...  \n",
       "1     Electronics › Accessories & Supplies › Audio &...  \n",
       "2     Tools & Home Improvement › Electrical › Wall P...  \n",
       "3     Home & Kitchen › Furniture › Living Room Furni...  \n",
       "4                                                   NaN  \n",
       "...                                                 ...  \n",
       "1638  Home & Kitchen › Home Décor Products › Candles...  \n",
       "1639  Electronics › Wearable Technology › Arm & Wris...  \n",
       "1640  Clothing, Shoes & Jewelry › Women › Shoes › Lo...  \n",
       "1641  Beauty & Personal Care › Hair Care › Hair Colo...  \n",
       "1642  Home & Kitchen › Furniture › Living Room Furni...  \n",
       "\n",
       "[1643 rows x 5 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6178bd9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>conversations</th>\n",
       "      <th>reward</th>\n",
       "      <th>source</th>\n",
       "      <th>instruction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[{'from': 'human', 'value': 'You are web shopp...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>human</td>\n",
       "      <td>i need a long clip-in hair extension which is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>[{'from': 'human', 'value': 'You are web shopp...</td>\n",
       "      <td>0.75</td>\n",
       "      <td>human</td>\n",
       "      <td>i need gluten free vegetarian smoked peppered ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>[{'from': 'human', 'value': 'You are web shopp...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>human</td>\n",
       "      <td>find me a high speed dual style package with  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>[{'from': 'human', 'value': 'You are web shopp...</td>\n",
       "      <td>0.75</td>\n",
       "      <td>human</td>\n",
       "      <td>i want to find xx-large black workout sweatpan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>[{'from': 'human', 'value': 'You are web shopp...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>human</td>\n",
       "      <td>i am looking for a grey sectional sofa for my ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1819</th>\n",
       "      <td>12036</td>\n",
       "      <td>[{'from': 'human', 'value': 'You are web shopp...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>gpt</td>\n",
       "      <td>i'm looking for a telescope with high power fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1820</th>\n",
       "      <td>12039</td>\n",
       "      <td>[{'from': 'human', 'value': 'You are web shopp...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>gpt</td>\n",
       "      <td>i am looking for a 3 vanity lights with  with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1821</th>\n",
       "      <td>12050</td>\n",
       "      <td>[{'from': 'human', 'value': 'You are web shopp...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>gpt</td>\n",
       "      <td>i need some grey living room pillow covers, an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1822</th>\n",
       "      <td>12077</td>\n",
       "      <td>[{'from': 'human', 'value': 'You are web shopp...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>gpt</td>\n",
       "      <td>i needed a 5-case gluten free multigrain table...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1823</th>\n",
       "      <td>12080</td>\n",
       "      <td>[{'from': 'human', 'value': 'You are web shopp...</td>\n",
       "      <td>0.75</td>\n",
       "      <td>gpt</td>\n",
       "      <td>i need a brown 6 wide sandal with ankle strap,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1824 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                      conversations  reward source  \\\n",
       "0         0  [{'from': 'human', 'value': 'You are web shopp...    1.00  human   \n",
       "1         3  [{'from': 'human', 'value': 'You are web shopp...    0.75  human   \n",
       "2         4  [{'from': 'human', 'value': 'You are web shopp...    1.00  human   \n",
       "3         8  [{'from': 'human', 'value': 'You are web shopp...    0.75  human   \n",
       "4         9  [{'from': 'human', 'value': 'You are web shopp...    1.00  human   \n",
       "...     ...                                                ...     ...    ...   \n",
       "1819  12036  [{'from': 'human', 'value': 'You are web shopp...    1.00    gpt   \n",
       "1820  12039  [{'from': 'human', 'value': 'You are web shopp...    1.00    gpt   \n",
       "1821  12050  [{'from': 'human', 'value': 'You are web shopp...    1.00    gpt   \n",
       "1822  12077  [{'from': 'human', 'value': 'You are web shopp...    1.00    gpt   \n",
       "1823  12080  [{'from': 'human', 'value': 'You are web shopp...    0.75    gpt   \n",
       "\n",
       "                                            instruction  \n",
       "0     i need a long clip-in hair extension which is ...  \n",
       "1     i need gluten free vegetarian smoked peppered ...  \n",
       "2     find me a high speed dual style package with  ...  \n",
       "3     i want to find xx-large black workout sweatpan...  \n",
       "4     i am looking for a grey sectional sofa for my ...  \n",
       "...                                                 ...  \n",
       "1819  i'm looking for a telescope with high power fo...  \n",
       "1820  i am looking for a 3 vanity lights with  with ...  \n",
       "1821  i need some grey living room pillow covers, an...  \n",
       "1822  i needed a 5-case gluten free multigrain table...  \n",
       "1823  i need a brown 6 wide sandal with ankle strap,...  \n",
       "\n",
       "[1824 rows x 5 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8a1c3f92-5a53-43c0-ba96-cc95f8f43b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "GOAL_RE   = re.compile(r\"(?im)^\\s*Goal\\s*:\\s*(.+?)\\s*$\")\n",
    "SCORE_RE  = re.compile(r\"Your score.*?:\\s*([0-9]+(?:\\.[0-9]+)?)\", re.I)\n",
    "def parse_self_traces(conversations: List[Dict[str,str]]) -> Tuple[List[Dict], Dict]:\n",
    "    \"\"\"\n",
    "    conversations: [{'role':'system'|'user'|'assistant','content':str}, ...]\n",
    "    Returns: (events, meta)  where events follow the unified schema used by score_webshop(...)\n",
    "    \"\"\"\n",
    "    events: List[Dict[str,Any]] = []\n",
    "    instruction = None\n",
    "    reward_val  = None\n",
    "\n",
    "    pending = None        # (type,arg) from assistant Action until we see the next user observation\n",
    "    first_total = None\n",
    "\n",
    "    for msg in conversations:\n",
    "        role = msg.get(\"role\",\"\")\n",
    "        text = str(msg.get(\"content\",\"\"))\n",
    "\n",
    "        # capture Goal once (it's repeated each assistant turn)\n",
    "        if instruction is None:\n",
    "            mg = GOAL_RE.search(text)\n",
    "            if mg: instruction = mg.group(1).strip()\n",
    "\n",
    "        # assistant proposes an Action inside <turn> blocks\n",
    "        if role == \"assistant\":\n",
    "            m = ACTION_RE.search(text)\n",
    "            if m:\n",
    "                pending = normalize_action(m.group(1), m.group(2))\n",
    "            continue\n",
    "\n",
    "        # user returns the observation (and sometimes final score)\n",
    "        if role == \"user\":\n",
    "            # if this is the obs following an assistant Action, commit the event\n",
    "            if pending:\n",
    "                ev_type, ev_arg = pending\n",
    "                ev = {\"type\": ev_type}\n",
    "                if ev_type == \"search\":    ev[\"query\"] = ev_arg\n",
    "                elif ev_type == \"open_item\": ev[\"asin\"] = ev_arg\n",
    "                elif ev_type == \"subpage\": ev[\"which\"] = ev_arg\n",
    "                # attach total results if present on this observation\n",
    "                if ev_type == \"search\":\n",
    "                    mt = TOTRES_RE.search(text)\n",
    "                    if mt and first_total is None:\n",
    "                        first_total = int(mt.group(1))\n",
    "                        ev[\"total_results\"] = first_total\n",
    "                events.append(ev)\n",
    "                pending = None\n",
    "\n",
    "            # optional: read reward\n",
    "            ms = SCORE_RE.search(text)\n",
    "            if ms:\n",
    "                try: reward_val = float(ms.group(1))\n",
    "                except: pass\n",
    "\n",
    "    # constraint heuristic from instruction text\n",
    "    constraints = 0\n",
    "    instr_l = norm_lower(instruction or \"\")\n",
    "    if re.search(r\"\\b(under|less than|lower than|at most|<=|\\$)\\b|\\d+\\.\\d{2}\", instr_l): constraints += 1\n",
    "    if re.search(r\"\\b(black|white|red|blue|green|gold|rose gold|silver|beige|pink)\\b\", instr_l): constraints += 1\n",
    "    if re.search(r\"\\b(\\d+\\s*(inch|in|oz|pack|pcs|piece|cm|mm))\\b\", instr_l): constraints += 1\n",
    "    if re.search(r\"\\b(natural|lightweight|easy to carry|waterproof|wireless|noise cancelling|carbon fiber|4-tier)\\b\", instr_l): constraints += 1\n",
    "\n",
    "    meta = {\n",
    "        \"instruction\": instruction or \"\",\n",
    "        \"constraints_count\": constraints,\n",
    "        \"reward\": reward_val,\n",
    "    }\n",
    "    return events, meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2d450cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "INSTR_RE = re.compile(\n",
    "    r\"(?is)\\bInstruction\\s*:\\s*\"\n",
    "    r\"(?:\\[SEP\\]\\s*)?\"             # optional opening [SEP]\n",
    "    r\"(.*?)\"                       # capture the instruction text (non-greedy)\n",
    "    r\"(?:\"                         # stop at one of:\n",
    "    r\"\\s*\\[SEP\\]\"                  #   closing [SEP]\n",
    "    r\"|\\n\\[[^\\]]+\\]\"               #   a bracketed control like [Search], [Back to Search]\n",
    "    r\"|$\"                          #   or end of string\n",
    "    r\")\"\n",
    ")\n",
    "\n",
    "def _norm_space(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", (s or \"\")).strip()\n",
    "\n",
    "def _norm_instr(s: str) -> str:\n",
    "    return _norm_space((s or \"\").lower())\n",
    "\n",
    "def parse_run_trace(path: str) -> tuple[str, list[tuple[str,str]], str]:\n",
    "    msgs = json.load(open(path))\n",
    "    run_id = pathlib.Path(path).stem\n",
    "    # if you inserted meta with {\"role\":\"meta\",\"content\":{\"id\":...}}, prefer that:\n",
    "    if msgs and msgs[0].get(\"role\") == \"meta\":\n",
    "        rid = msgs[0][\"content\"].get(\"id\")\n",
    "        if rid:\n",
    "            run_id = str(rid)\n",
    "\n",
    "    instruction = None\n",
    "    pending = None\n",
    "    seq: list[tuple[str,str]] = []\n",
    "    for m in msgs:\n",
    "        role = m.get(\"role\",\"\")\n",
    "        text = str(m.get(\"content\",\"\"))\n",
    "        if role == \"user\":\n",
    "            if instruction is None:\n",
    "                mi = INSTR_RE.search(text)\n",
    "                if mi:\n",
    "                    instruction = mi.group(1).strip()\n",
    "            if pending:\n",
    "                seq.append(pending)\n",
    "                pending = None\n",
    "    return _norm_instr(instruction or \"\"), seq, run_id[6:], msgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ce365bd9-23b7-4bd0-9258-31235a513f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, math, json\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "# ---------- Shared helpers ----------\n",
    "ASIN_RE = re.compile(r\"\\bB0[A-Z0-9]{8}\\b\", re.I)\n",
    "TOTRES_RE = re.compile(r\"Total results:\\s*(\\d+)\", re.I)\n",
    "INSTR_RE  = re.compile(r\"(?is)\\bInstruction\\s*:\\s*(?:\\[SEP\\]\\s*)?(.*?)(?:\\s*\\[SEP\\]|\\n\\[[^\\]]+\\]|$)\")\n",
    "THOUGHT_RE = re.compile(r\"(?ims)^\\s*Thought:\\s*(.*?)(?=\\n\\s*Action:|$)\")\n",
    "ACTION_RE  = re.compile(r\"(?im)^\\s*Action:\\s*(search|click|choose)\\[(.+?)\\]\\s*$\")\n",
    "def norm_space(s:str)->str: return re.sub(r\"\\s+\",\" \", (s or \"\")).strip()\n",
    "def norm_lower(s:str)->str: return norm_space(s).lower()\n",
    "\n",
    "def normalize_action(verb: str, arg: str) -> Tuple[str, str]:\n",
    "    v = (verb or \"\").lower().strip()\n",
    "    if v == \"choose\": v = \"click\"\n",
    "    a = (arg or \"\").strip()\n",
    "    # map common controls\n",
    "    low = a.lower()\n",
    "    if v == \"click\":\n",
    "        if low in {\"buy now\", \"buy\"}: return (\"buy\",\"\")\n",
    "        if low in {\"next >\", \"next>\"}: return (\"nav\",\"next\")\n",
    "        if low in {\"< prev\", \"prev\"}:  return (\"nav\",\"prev\")\n",
    "        if low in {\"back to search\", \"back\"}: return (\"nav\",\"back\")\n",
    "        if low in {\"description\", \"features\", \"reviews\"}: return (\"subpage\", low)\n",
    "        # ASIN?\n",
    "        m = ASIN_RE.search(a)\n",
    "        if m: return (\"open_item\", m.group(0).upper())\n",
    "        # option or other button\n",
    "        return (\"option\", norm_lower(a))\n",
    "    elif v == \"search\":\n",
    "        return (\"search\", norm_lower(a))\n",
    "    return (v, norm_lower(a))\n",
    "\n",
    "# ---------- Parser A: structured (human/expert or scripted) ----------\n",
    "def parse_structured(traj: Dict[str, Any]) -> Tuple[List[Dict], Dict]:\n",
    "    \"\"\"\n",
    "    Returns (events, meta)\n",
    "    events: [{type, query/asin/which, total_results?}, ...]\n",
    "    meta:   {'instruction': str, 'constraints_count': int}\n",
    "    \"\"\"\n",
    "    goal = traj.get(\"goal\", {}) or {}\n",
    "    instr = goal.get(\"instruction_text\", \"\")\n",
    "    constraints = 0\n",
    "    if goal.get(\"price_upper\") is not None: constraints += 1\n",
    "    constraints += len(goal.get(\"attributes\", []) or [])\n",
    "    constraints += len(goal.get(\"goal_options\", []) or [])\n",
    "\n",
    "    events, first_total = [], None\n",
    "    last_opts = {}\n",
    "\n",
    "    for st in traj.get(\"steps\", []):\n",
    "        a = st.get(\"action\", {}) or {}\n",
    "        typ = a.get(\"type\",\"\")\n",
    "        # search\n",
    "        if typ == \"search\":\n",
    "            q = a.get(\"args\",{}).get(\"keywords\") or []\n",
    "            q = \" \".join(map(str,q))\n",
    "            ev = {\"type\":\"search\",\"query\":norm_lower(q)}\n",
    "            # if candidates list is provided for this step/state\n",
    "            cand = (st.get(\"state\") or {}).get(\"candidates\")\n",
    "            if isinstance(cand, list) and cand and first_total is None:\n",
    "                first_total = len(cand)\n",
    "                ev[\"total_results\"] = first_total\n",
    "            events.append(ev)\n",
    "        # item open (from search or click)\n",
    "        elif typ == \"open_item\":\n",
    "            # sometimes ASIN not given, but state updated elsewhere\n",
    "            # we still count an open\n",
    "            events.append({\"type\":\"open_item\"})\n",
    "        # buy\n",
    "        elif typ == \"buy\":\n",
    "            events.append({\"type\":\"buy\"})\n",
    "        # option detection: when state.options grows\n",
    "        opts = (st.get(\"state\") or {}).get(\"options\") or {}\n",
    "        if len(opts) > len(last_opts):\n",
    "            events.append({\"type\": \"option\"})\n",
    "        last_opts = opts\n",
    "\n",
    "    meta = {\"instruction\": instr, \"constraints_count\": constraints}\n",
    "    return events, meta\n",
    "\n",
    "# ---------- Parser B: HF-style dialogue (Action/Observation) ----------\n",
    "def parse_hf_dialog(conversations: List[Dict[str, str]]) -> Tuple[List[Dict], Dict]:\n",
    "    \"\"\"\n",
    "    Returns (events, meta). meta.constraints_count is a heuristic.\n",
    "    \"\"\"\n",
    "    instruction = None\n",
    "    events, pending = [], None\n",
    "    first_total = None\n",
    "\n",
    "    for msg in conversations:\n",
    "        who = msg.get(\"from\",\"\")\n",
    "        text = str(msg.get(\"value\",\"\"))\n",
    "\n",
    "        if who == \"gpt\":\n",
    "            m = ACTION_RE.search(text)\n",
    "            if m:\n",
    "                pending = normalize_action(m.group(1), m.group(2))\n",
    "        elif who == \"human\":\n",
    "            if instruction is None:\n",
    "                mi = INSTR_RE.search(text)\n",
    "                if mi: instruction = mi.group(1).strip()\n",
    "            if pending:\n",
    "                ev_type, ev_arg = pending\n",
    "                ev = {\"type\":ev_type}\n",
    "                if ev_type == \"search\": ev[\"query\"] = ev_arg\n",
    "                if ev_type == \"open_item\": ev[\"asin\"] = ev_arg\n",
    "                if ev_type == \"subpage\":   ev[\"which\"] = ev_arg\n",
    "                # parse total results from this observation (if present)\n",
    "                if ev_type == \"search\":\n",
    "                    m = TOTRES_RE.search(text)\n",
    "                    if m and first_total is None:\n",
    "                        first_total = int(m.group(1))\n",
    "                        ev[\"total_results\"] = first_total\n",
    "                events.append(ev)\n",
    "                pending = None\n",
    "\n",
    "    # crude constraint heuristic from instruction\n",
    "    constraints = 0\n",
    "    instr_l = norm_lower(instruction or \"\")\n",
    "    if re.search(r\"\\b(under|less than|lower than|at most|<=|\\$)\\b|\\d+\\.\\d{2}\", instr_l): constraints += 1  # price-ish\n",
    "    # count a few attribute-y tokens (color/size/length)\n",
    "    if re.search(r\"\\b(black|white|red|blue|green|gold|rose gold|silver|beige|pink)\\b\", instr_l): constraints += 1\n",
    "    if re.search(r\"\\b(\\d+\\s*(inch|in|oz|pack|pcs|piece|cm|mm))\\b\", instr_l): constraints += 1\n",
    "    if re.search(r\"\\b(natural|lightweight|easy to carry|waterproof|wireless|noise cancelling|4-tier)\\b\", instr_l): constraints += 1\n",
    "\n",
    "    meta = {\"instruction\": instruction or \"\", \"constraints_count\": constraints}\n",
    "    return events, meta\n",
    "\n",
    "# ---------- Difficulty scorer ----------\n",
    "W_WS = {\n",
    "    \"search\": 1.0,\n",
    "    \"reformulate\": 1.2,\n",
    "    \"open_item\": 0.8,\n",
    "    \"paginate\": 0.8,      # next/prev/back\n",
    "    \"subpage\": 0.5,\n",
    "    \"option\": 1.4,\n",
    "    \"unique_items\": 0.5,\n",
    "    \"steps\": 0.2,\n",
    "    \"total_results_log\": 0.4,\n",
    "    \"constraints\": 0.8,\n",
    "}\n",
    "\n",
    "def score_webshop(events: List[Dict], meta: Dict) -> Dict[str, Any]:\n",
    "    n_search = 0\n",
    "    queries = []\n",
    "    n_open = 0\n",
    "    n_sub = 0\n",
    "    n_opt = 0\n",
    "    n_pag = 0\n",
    "    n_buy = 0\n",
    "    asins = set()\n",
    "    first_total = None\n",
    "\n",
    "    for ev in events:\n",
    "        t = ev.get(\"type\")\n",
    "        if t == \"search\":\n",
    "            n_search += 1\n",
    "            q = ev.get(\"query\",\"\")\n",
    "            if q: queries.append(q)\n",
    "            if first_total is None and \"total_results\" in ev:\n",
    "                first_total = int(ev[\"total_results\"])\n",
    "        elif t == \"open_item\":\n",
    "            n_open += 1\n",
    "            if ev.get(\"asin\"): asins.add(ev[\"asin\"])\n",
    "        elif t == \"subpage\":\n",
    "            n_sub += 1\n",
    "        elif t == \"option\":\n",
    "            n_opt += 1\n",
    "        elif t == \"nav\":\n",
    "            n_pag += 1\n",
    "        elif t == \"buy\":\n",
    "            n_buy += 1\n",
    "\n",
    "    # queries that are genuinely different (reformulations)\n",
    "    distinct = []\n",
    "    for q in queries:\n",
    "        if not distinct or q != distinct[-1]:\n",
    "            distinct.append(q)\n",
    "    n_reform = max(0, len(distinct) - 1)\n",
    "\n",
    "    # build the score\n",
    "    score = (\n",
    "        W_WS[\"search\"] * n_search +\n",
    "        W_WS[\"reformulate\"] * n_reform +\n",
    "        W_WS[\"open_item\"] * n_open +\n",
    "        W_WS[\"paginate\"] * n_pag +\n",
    "        W_WS[\"subpage\"] * n_sub +\n",
    "        W_WS[\"option\"] * n_opt +\n",
    "        W_WS[\"unique_items\"] * len(asins) +\n",
    "        W_WS[\"steps\"] * len(events) +\n",
    "        W_WS[\"total_results_log\"] * math.log1p(first_total or 10) +\n",
    "        W_WS[\"constraints\"] * int(meta.get(\"constraints_count\", 0))\n",
    "    )\n",
    "    # print(n_search ,n_reform , \n",
    "    #       n_open,n_pag ,n_sub, n_opt, len(asins), len(events), math.log1p(first_total or 10),int(meta.get(\"constraints_count\", 0)))\n",
    "    # print(score)\n",
    "    return {\n",
    "        \"score\": round(float(score), 3),\n",
    "        \"n_search\": n_search,\n",
    "        \"n_reformulate\": n_reform,\n",
    "        \"n_open_item\": n_open,\n",
    "        \"n_subpage\": n_sub,\n",
    "        \"n_option\": n_opt,\n",
    "        \"n_paginate\": n_pag,\n",
    "        \"n_unique_items\": len(asins),\n",
    "        \"n_steps\": len(events),\n",
    "        \"total_results_first\": first_total,\n",
    "        \"constraints_count\": int(meta.get(\"constraints_count\", 0)),\n",
    "        \"bought\": bool(n_buy),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "821af608-bd71-461a-8e17-8483fac8251a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadata = pd.read_csv(\"data/train_metadata.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e8c1aff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19256/551998327.py:14: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  category = train_metadata[\"category\"][number]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# df_train = df_train[df_train.success == 1]\n",
    "# df_train.reset_index(inplace=True, drop=True)\n",
    "df_train[\"expert_difficulty\"] = None\n",
    "df_train[\"category\"] = None\n",
    "\n",
    "for idx, row in df_train.iterrows():\n",
    "    file = row.trace_file\n",
    "    match = re.search(r'_(\\d+)\\.json$', file)\n",
    "    number = int(match.group(1))\n",
    "    # if row.success == 1:\n",
    "    file_path = PROMPT_BASE_DIR+file\n",
    "    category = train_metadata[\"category\"][number]\n",
    "    \n",
    "    instr, seq, json_id, msgs = parse_run_trace(file_path)\n",
    "    \n",
    "    # category = METADATA_DIR+file+\"l\"\n",
    "    # with open(category) as f:\n",
    "    #     json_list = list(f)\n",
    "    #     category = ast.literal_eval(json_list[0])\n",
    "    # category = category[\"category\"]\n",
    "    \n",
    "    if category == \"grocery\":\n",
    "        category = \"food\"\n",
    "    if category == \"garden\":\n",
    "        category = \"furniture\"\n",
    "    df_train.at[idx, \"category\"] = category\n",
    "    events, meta = parse_self_traces(msgs)\n",
    "    difficulty = score_webshop(events, meta)['score'] \n",
    "    df_train.at[idx, 'expert_difficulty'] = difficulty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3dc61eea-4f8b-44b0-96d1-27fc65bddd70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i need caxxa amber glass fine mist spray bottles, size 12 refillable containers, and price lower than 50.00 dollars'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_expert.instruction[99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5f619e8c-4cae-4110-afed-aef2966601de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['beauty', 'fashion', 'furniture', 'electronics', 'food'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.category.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "824d358b-584b-4371-96b3-d9fa95954ef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "beauty         262\n",
       "fashion        251\n",
       "food           239\n",
       "furniture      229\n",
       "electronics    219\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4577bdee-929e-493f-8eec-ec0543c56a1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "df_train[\"expert_traces\"] = \"training_set\"\n",
    "df_train[\"orig_trace\"] = None\n",
    "\n",
    "\n",
    "for idx, file in enumerate(df_train.trace_file):\n",
    "    file_path = PROMPT_BASE_DIR+file\n",
    "    instr, seq, json_id, msgs = parse_run_trace(file_path)\n",
    "    df_train.at[idx, \"orig_trace\"] = msgs\n",
    "    \n",
    "    train_reward = df_train.iloc[idx].total_reward \n",
    "    mask = df_expert[\"instruction\"].eq(instr)\n",
    "    mask2 = df_eto[\"instruction\"].eq(instr)\n",
    "    reward = 0\n",
    "    reward2 = 0\n",
    "    if mask.any():\n",
    "        i = df_expert.index[mask][0]          # index label\n",
    "        # print(\"found in webshop experts:\", instr, \"at index\", i)\n",
    "        expert_trace = df_expert.at[i, \"file\"]  # safer than .values[0]\n",
    "        reward = df_expert.at[i, \"reward\"]\n",
    "        # if df_train.iloc[idx].total_reward <= df_expert.at[i, \"reward\"]:\n",
    "        #     # print('better')\n",
    "        #     df_train[\"expert_trace\"] = expert_trace\n",
    "\n",
    "    if mask2.any():\n",
    "        i = df_eto.index[mask2][0]          # index label\n",
    "        # print(\"found in webshop experts:\", instr, \"at index\", i)\n",
    "        # print(i, df)\n",
    "        expert_trace2 = df_eto.at[i, \"conversations\"]  # safer than .values[0]\n",
    "        reward2 = df_eto.at[i, \"reward\"]\n",
    "\n",
    "    # print(reward)\n",
    "    if train_reward < reward:\n",
    "        # print(train_reward, reward)\n",
    "        traj = ast.literal_eval(expert_trace)\n",
    "        events, meta = parse_structured(traj)\n",
    "        difficulty = score_webshop(events, meta)['score'] \n",
    "        df_train.at[idx, 'expert_difficulty'] = difficulty\n",
    "        df_train.at[idx, 'expert_traces'] = \"Human Expert\" \n",
    "        # break\n",
    "    elif train_reward < reward2:\n",
    "        # print(train_reward, reward2)\n",
    "        events, meta = parse_hf_dialog(expert_trace2)\n",
    "        difficulty2 = score_webshop(events, meta)['score']        \n",
    "        df_train.at[idx, 'expert_difficulty'] = difficulty2\n",
    "        df_train.at[idx, 'expert_traces'] = \"ETO\"\n",
    "        \n",
    "# df_train[\"expert_trace\"] = expert_traces\n",
    "# df_train[\"expert_difficulty\"] = expert_difficulty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2e27a00b-b6c2-42de-847a-2526025bc07f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1200"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train.dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "37e57360-5d14-4e18-abde-88d89905ebfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "expert_traces\n",
       "training_set    718\n",
       "Human Expert     27\n",
       "ETO              17\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[df_train.success==1.0].expert_traces.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e79e4fc6-a01f-4beb-b7e5-699d61f8bf1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(17.820634703196344)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[df_train.total_reward==0].expert_difficulty.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d59b6688-a0e5-46e3-aaec-8c70b5efcfbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "beauty         87\n",
       "fashion        81\n",
       "electronics    69\n",
       "food           62\n",
       "furniture      53\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[df_train.total_reward==1].category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f896092f-a1ad-46f8-aed4-e6f87c0edf24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(6.453681818181809)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[df_train.total_reward==1].expert_difficulty.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "23c0d615-28e4-4b58-a9d9-046d9430e5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train.to_csv(\"data/react_train_set_difficulty_scoring.csv\", index=False)\n",
    "\n",
    "\n",
    "# df_train.to_csv(\"data/state_train_difficulty_selfgenerated.csv\", index=False)\n",
    "df_train.to_csv(\"data/react_train_difficulty_selfgenerated.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e602f8c-4996-4c3e-8943-e02e94ec1d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/workspace/miniforge3/envs/finetune/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import ast\n",
    "import json\n",
    "import pathlib\n",
    "import glob\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from typing import List, Dict, Any, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4daa113-c731-48a8-90c2-37e96c983850",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_few_shot_examples(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Keeps the first line ('Interact with a household...') and removes the few-shot examples\n",
    "    between 'Here are 2 examples:' and the last 'Your task is to:'.\n",
    "    \"\"\"\n",
    "    # Extract the first line (system instruction)\n",
    "    first_line, *rest = prompt.split(\"\\n\", 1)\n",
    "    rest_text = rest[0] if rest else \"\"\n",
    "\n",
    "    #########################################\n",
    "\n",
    "    # Split by all occurrences of 'Your task is to:'\n",
    "    parts = rest_text.split(\"\\nWebShop \\nInstruction:  \\n\")\n",
    "    if len(parts) < 2:\n",
    "        return prompt  # Nothing to strip\n",
    "\n",
    "    # Keep only the last task and everything that follows\n",
    "    cleaned = first_line.strip() + \"\\nWebShop \\nInstruction:  \\n\" + parts[-1]\n",
    "\n",
    "    return cleaned.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "222cd10e-7942-4a61-96f3-a7b637218300",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_whole_trace(example):\n",
    "    trace = ast.literal_eval(example.orig_trace)\n",
    "    # print(trace[1])\n",
    "    # Prompt = system + initial user\n",
    "    sys_msg  = next(m[\"content\"] for m in trace if m[\"role\"]==\"system\")\n",
    "    usr_msg  = next(m[\"content\"] for m in trace if m[\"role\"]==\"user\")\n",
    "\n",
    "    # print(usr_msg)\n",
    "    prompt   = sys_msg + \"\\n\" + usr_msg + \"\\n\"\n",
    "    # Completion = *everything else*, in order\n",
    "    rest     = trace[2:]\n",
    "    completion = \"\".join(m[\"content\"] + \"\\n\" for m in rest)\n",
    "    # add end‐of‐sequence token if needed\n",
    "    if not completion.endswith(\"\"):\n",
    "        completion += \"\"\n",
    "        \n",
    "    match = re.findall(\"Nothing happens.\", completion)\n",
    "    # if len(match) > 0:\n",
    "    #     print(len(match))\n",
    "    \n",
    "    return {\"prompt\": prompt, \"completion\": completion, \"nothing_occ\":len(match)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ce01cbf-6281-484b-8369-817f10e842bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "expert_traces\n",
       "training_set    1098\n",
       "Human Expert      70\n",
       "ETO               32\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/rag_train_set_difficulty_scoring.csv\") #RAG DATASET\n",
    "df.expert_traces.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8ecd0c2-924f-40a2-be7f-139b7aff5e59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**************************************************\n",
      "Raw generated samples: 380\n",
      "**************************************************\n",
      "\n",
      "Final usable examples: 380\n",
      "Average Reward and Count by category:\n",
      "             reward_average  count\n",
      "category                          \n",
      "beauty                  1.0    110\n",
      "electronics             1.0     74\n",
      "fashion                 1.0    115\n",
      "food                    1.0     77\n",
      "furniture               1.0     72\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import ast, json, re, pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# ------------------ 0) Config ------------------\n",
    "# INPUT_CSV      = \"data/train_set_difficulty_scoring.csv\"\n",
    "# INPUT_CSV      = \"data/react_train_set_difficulty_scoring.csv\"\n",
    "INPUT_CSV = \"data/state_train_difficulty_selfgenerated.csv\" #WITH RAG!!\n",
    "# INPUT_CSV = \"data/react_train_difficulty_selfgenerated.csv\" #WITH RAG!!\n",
    "\n",
    "# MODEL_NAME     = \"Qwen/Qwen2.5-14B-Instruct\"\n",
    "MODEL_NAME     = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "USE_HANDPICKED = False\n",
    "# KEEP_MODE options:\n",
    "#   \"first_assistant_only\"\n",
    "#   \"last_assistant_only\"\n",
    "#   \"every_user_assistant_pair\"       # prompt = system + that single user only\n",
    "#   \"every_pair_with_history\"         # NEW: prompt = full history up to each assistant\n",
    "#   \"full_episode_completion\"\n",
    "#   \"midpoint_next_step\"\n",
    "#   \"last_step_with_history\"\n",
    "KEEP_MODE      = \"full_episode_completion\"\n",
    "\n",
    "MIDPOINT = \"auto\"   # for midpoint mode\n",
    "TASK_SENTINEL  = \"Here is the task\"\n",
    "\n",
    "# ------------------ 1) Load & basic filtering ------------------\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "df = df[df[\"success\"] == 1.0]\n",
    "df = df[df[\"total_reward\"] >= 1].reset_index(drop=True)\n",
    "\n",
    "# ------------------ 2) Helpers ------------------\n",
    "def _json_or_ast_load(s: str):\n",
    "    if isinstance(s, (list, dict)): return s\n",
    "    s = s.strip()\n",
    "    try: return json.loads(s)\n",
    "    except Exception: return ast.literal_eval(s)\n",
    "\n",
    "def strip_fewshot_from_system(sys_text: str) -> str:\n",
    "    if not sys_text: return sys_text\n",
    "    if TASK_SENTINEL in sys_text:\n",
    "        head = sys_text.split(\"\\n\", 1)[0]\n",
    "        tail = sys_text.split(TASK_SENTINEL, 1)[1]\n",
    "        return (head + \"\\n\\n\" + TASK_SENTINEL + tail).strip()\n",
    "    return sys_text.strip()\n",
    "\n",
    "def apply_chat_prompt(messages):\n",
    "    # Qwen chat template; ends right before assistant generation\n",
    "    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "def build_prompt_from_history(trace, upto_exclusive_idx: int):\n",
    "    \"\"\"Prompt = system + full (user/assistant) history strictly before upto_exclusive_idx.\"\"\"\n",
    "    sys_msg = next((m.get(\"content\",\"\") for m in trace if m.get(\"role\")==\"system\"), \"\")\n",
    "    sys_msg = strip_fewshot_from_system(sys_msg)\n",
    "    msgs = []\n",
    "    if sys_msg:\n",
    "        msgs.append({\"role\": \"system\", \"content\": sys_msg})\n",
    "    for i in range(upto_exclusive_idx):\n",
    "        role = trace[i].get(\"role\")\n",
    "        if role in (\"user\", \"assistant\"):\n",
    "            content = (trace[i].get(\"content\") or \"\").strip()\n",
    "            if content:\n",
    "                msgs.append({\"role\": role, \"content\": content})\n",
    "    return apply_chat_prompt(msgs)\n",
    "\n",
    "def user_assistant_pairs(trace):\n",
    "    \"\"\"List of (user_idx, next_assistant_idx) pairs in chronological order.\"\"\"\n",
    "    pairs, n = [], len(trace)\n",
    "    for i, m in enumerate(trace):\n",
    "        if m.get(\"role\") != \"user\": continue\n",
    "        j = next((k for k in range(i+1, n) if trace[k].get(\"role\")==\"assistant\"), None)\n",
    "        if j is not None: pairs.append((i, j))\n",
    "    return pairs\n",
    "\n",
    "def split_prompt_completion_from_trace(trace, keep_mode=\"first_assistant_only\"):\n",
    "    \"\"\"\n",
    "    Returns list[{\"prompt\": str, \"completion\": str}]\n",
    "    \"\"\"\n",
    "    sys_msg = next((m.get(\"content\",\"\") for m in trace if m.get(\"role\")==\"system\"), \"\")\n",
    "    sys_msg = strip_fewshot_from_system(sys_msg)\n",
    "\n",
    "    # ---------- NEW: every_pair_with_history ----------\n",
    "    if keep_mode == \"every_pair_with_history\":\n",
    "        pairs = user_assistant_pairs(trace)\n",
    "        out = []\n",
    "        for ui, ai in pairs:\n",
    "            # prompt: full history up to the assistant (exclusive)\n",
    "            prompt = build_prompt_from_history(trace, upto_exclusive_idx=ai)\n",
    "            asst = (trace[ai].get(\"content\") or \"\").strip()\n",
    "            if not asst: continue\n",
    "            out.append({\"prompt\": prompt, \"completion\": asst})\n",
    "        return out\n",
    "\n",
    "    # existing: every_user_assistant_pair (prompt only that user, no earlier turns)\n",
    "    if keep_mode == \"every_user_assistant_pair\":\n",
    "        pairs = user_assistant_pairs(trace)\n",
    "        out = []\n",
    "        for ui, ai in pairs:\n",
    "            usr = (trace[ui].get(\"content\") or \"\").strip()\n",
    "            asst = (trace[ai].get(\"content\") or \"\").strip()\n",
    "            if not usr or not asst: continue\n",
    "            msgs = []\n",
    "            if sys_msg: msgs.append({\"role\": \"system\", \"content\": sys_msg})\n",
    "            msgs.append({\"role\": \"user\", \"content\": usr})\n",
    "            prompt = apply_chat_prompt(msgs)\n",
    "            out.append({\"prompt\": prompt, \"completion\": asst})\n",
    "        return out\n",
    "\n",
    "    # ----- single-sample modes below -----\n",
    "    pairs = user_assistant_pairs(trace)\n",
    "    if not pairs: return []\n",
    "\n",
    "    try:\n",
    "        first_user_idx = next(i for i, m in enumerate(trace) if m.get(\"role\")==\"user\")\n",
    "    except StopIteration:\n",
    "        return []\n",
    "\n",
    "    if keep_mode == \"first_assistant_only\":\n",
    "        usr_msg = (trace[first_user_idx].get(\"content\") or \"\").strip()\n",
    "        if not usr_msg: return []\n",
    "        prompt = apply_chat_prompt(([{\"role\":\"system\",\"content\":sys_msg}] if sys_msg else []) +\n",
    "                                   [{\"role\":\"user\",\"content\":usr_msg}])\n",
    "        remainder = trace[first_user_idx+1:]\n",
    "        first_asst = next((m.get(\"content\",\"\").rstrip() for m in remainder if m.get(\"role\")==\"assistant\"), \"\")\n",
    "        return [{\"prompt\": prompt, \"completion\": first_asst or \"\"}]\n",
    "\n",
    "    if keep_mode == \"last_assistant_only\":\n",
    "        ui, ai = pairs[-1]\n",
    "        usr = (trace[ui].get(\"content\") or \"\").strip()\n",
    "        asst = (trace[ai].get(\"content\") or \"\").strip()\n",
    "        if not usr or not asst: return []\n",
    "        prompt = apply_chat_prompt(([{\"role\":\"system\",\"content\":sys_msg}] if sys_msg else []) +\n",
    "                                   [{\"role\":\"user\",\"content\":usr}])\n",
    "        return [{\"prompt\": prompt, \"completion\": asst}]\n",
    "\n",
    "    if keep_mode == \"last_step_with_history\":\n",
    "        ui, ai = pairs[-1]\n",
    "        prompt = build_prompt_from_history(trace, upto_exclusive_idx=ai)\n",
    "        asst = (trace[ai].get(\"content\") or \"\").strip()\n",
    "        if not asst: return []\n",
    "        return [{\"prompt\": prompt, \"completion\": asst}]\n",
    "\n",
    "    if keep_mode == \"midpoint_next_step\":\n",
    "        idx = len(pairs)//2 if MIDPOINT == \"auto\" else int(MIDPOINT)\n",
    "        idx = max(0, min(idx, len(pairs)-1))\n",
    "        ui, ai = pairs[idx]\n",
    "        prompt = build_prompt_from_history(trace, upto_exclusive_idx=ai)\n",
    "        asst = (trace[ai].get(\"content\") or \"\").strip()\n",
    "        if not asst: return []\n",
    "        return [{\"prompt\": prompt, \"completion\": asst}]\n",
    "\n",
    "    # Fallback: full_episode_completion after first user\n",
    "    usr_msg = (trace[first_user_idx].get(\"content\") or \"\").strip()\n",
    "    if not usr_msg: return []\n",
    "    prompt = apply_chat_prompt(([{\"role\":\"system\",\"content\":sys_msg}] if sys_msg else []) +\n",
    "                               [{\"role\":\"user\",\"content\":usr_msg}])\n",
    "    remainder = trace[first_user_idx+1:]\n",
    "    parts = []\n",
    "    for m in remainder:\n",
    "        role = m.get(\"role\", \"assistant\")\n",
    "        content = (m.get(\"content\") or \"\").rstrip()\n",
    "        if content: parts.append(f\"{role}: {content}\")\n",
    "    return [{\"prompt\": prompt, \"completion\": \"\\n\".join(parts).strip()}]\n",
    "\n",
    "def calc_successes(df_in):\n",
    "    if \"category\" in df_in.columns and \"total_reward\" in df_in.columns:\n",
    "        grouped = df_in.groupby(\"category\")[\"total_reward\"]\n",
    "        success_stats = grouped.agg(\n",
    "            reward_average=lambda x: x.mean(),\n",
    "            count=\"count\"\n",
    "        ).round(2)\n",
    "        print(\"Average Reward and Count by category:\")\n",
    "        print(success_stats.to_string())\n",
    "\n",
    "# ------------------ 3) Tokenizer ------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "try: tokenizer.truncation_side = \"left\"\n",
    "except: pass\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# ------------------ 4) Build prompt/completion(s) ------------------\n",
    "def row_to_records(row):\n",
    "    trace = _json_or_ast_load(row.get(\"orig_trace\", \"[]\"))\n",
    "    if not isinstance(trace, list) or not trace: return []\n",
    "    samples = split_prompt_completion_from_trace(trace, keep_mode=KEEP_MODE)\n",
    "    recs = []\n",
    "    for s in samples:\n",
    "        prompt = s.get(\"prompt\") or \"\"\n",
    "        completion = s.get(\"completion\") or \"\"\n",
    "        back_click = \"click[Back to Search]\"\n",
    "        # if (not completion) or (\"Invalid action!\" in completion) or (back_click in completion):\n",
    "        if (not completion) or (\"Invalid action!\" in completion):\n",
    "            continue\n",
    "        recs.append({\"prompt\": prompt, \"completion\": completion})\n",
    "    return recs\n",
    "\n",
    "all_records = []\n",
    "for _, row in df.iterrows():\n",
    "    all_records.extend(row_to_records(row))\n",
    "\n",
    "print(\"\\n\" + \"*\"*50)\n",
    "print(f\"Raw generated samples: {len(all_records)}\")\n",
    "print(\"*\"*50 + \"\\n\")\n",
    "\n",
    "df_out = pd.DataFrame.from_records(all_records)\n",
    "\n",
    "# ------------------ 5) Optional handpicked top-20 per category ------------------\n",
    "if USE_HANDPICKED:\n",
    "    raise NotImplementedError(\"Handpicked selection should be applied BEFORE expansion.\")\n",
    "\n",
    "print(f\"Final usable examples: {len(df_out)}\")\n",
    "calc_successes(df)  # episode-level stats\n",
    "\n",
    "# ------------------ 6) Build HF dataset ------------------\n",
    "dataset = Dataset.from_pandas(df_out[[\"prompt\", \"completion\"]].copy())\n",
    "hf_ds = dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77e09700-64cf-42c5-ad12-b390e63fdff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nYou are a shopping assistant. Buy the closest item to the instruction provided. \\n\\nHere is the task.<|im_end|>\\n<|im_start|>user\\nWebShop \\nInstruction:  \\ni am interested in a high quality brush set, and price lower than 30.00 dollars \\n[Search]<|im_end|>\\n<|im_start|>assistant\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_out.prompt[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e311c65a-aaf9-4659-99df-f18f2acf1481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'assistant:  <turn>\\nGoal: Buy a high quality brush set, and price lower than 30.00 dollars\\nCurrent Location: Search Home Page\\nCurrent Selection: None\\nAction: search[high quality brush set <30]\\nuser: \\n[Back to Search] \\nPage 1 (Total results: 50) \\n[Next >] \\n[B07HMRCPFQ] \\nClearance! Lmtime High-End 18 pcs Makeup Brush Set tools Make-up Toiletry Kit Wool Make Up Brush Set (Purple) \\n$2.99 \\n[B07PWW6PKQ] \\nBEILI Makeup Brushes 30Pcs Professional Makeup Brush Set Premium Pink Vegan Synthetic Kabuki Foundation Blending Brush Face Powder Blush Concealers Eye Shadows Make Up Brush Set (White/Rose Gold) \\n$42.99 \\n[B01G6AUR32] \\nvela.yue Precise Crease Brush Eyes Tapered Blending Contour Makeup Tool \\n$5.99\\nassistant:  <turn>\\nGoal: Buy a high quality brush set, and price lower than 30.00 dollars\\nCurrent Location: Search Results Page, page 1\\nCurrent Selection: None\\nAction: click[B07HMRCPFQ]\\nuser: \\n[Back to Search] \\n[< Prev] \\ncolor [black][pink][purple]\\nClearance! Lmtime High-End 18 pcs Makeup Brush Set tools Make-up Toiletry Kit Wool Make Up Brush Set (Purple) \\nPrice: $2.99 \\nRating: N.A. \\n[Description] \\n[Features] \\n[Reviews] \\n[Buy Now]\\nassistant:  \\n<turn>\\nGoal: Buy a high quality brush set, and price lower than 30.00 dollars\\nCurrent Location: Item B07HMRCPFQ Page\\nCurrent Selection: None\\nAction: click[purple]\\nuser: You have clicked purple.\\nassistant:  \\n\\n<turn>\\nGoal: Buy a high quality brush set, and price lower than 30.00 dollars\\nCurrent Location: Item B07HMRCPFQ Page\\nCurrent Selection: purple\\nAction: click[Buy Now]\\nuser: Your score (min 0.0, max 1.0): 1.0'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_out.completion[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "35ae1453-0995-47ff-b2ca-24c4d41f502f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built trajectories: 444\n",
      "<|im_start|>system\n",
      "You are a shopping assistant. Buy the closest item to the instruction provided. \n",
      "\n",
      "Here is the task.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "reset<|im_end|>\n",
      "<|im_start|>user\n",
      "WebShop \n",
      "Instruction:  \n",
      "i am interested in a high quality brush set, and price lower than 50.00 dollars \n",
      "[Search]<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<turn>\n",
      "Goal: Buy a high quality brush set, and price lower than 50.00 dollars\n",
      "Current Location: Search Home Page\n",
      "Current Selection: None\n",
      "Action: search[high quality brush\n",
      "[{\"start_char\": 151, \"end_char\": 156, \"correct\": true, \"idx\": 0}, {\"start_char\": 330, \"end_char\": 505, \"correct\": true, \"idx\": 1}, {\"start_char\": 1103, \"end_char\": 1276, \"correct\": true, \"idx\": 2}, {\"start_char\": 1607, \"end_char\": 1770, \"correct\": true, \"idx\": 3}]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import ast, json, re, pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# ------------------ 0) Config ------------------\n",
    "INPUT_CSV      = \"data/rag_train_set_difficulty_scoring.csv\"\n",
    "MODEL_NAME     = \"Qwen/Qwen2.5-14B-Instruct\"\n",
    "\n",
    "USE_HANDPICKED = False\n",
    "TASK_SENTINEL  = \"Here is the task\"\n",
    "\n",
    "# ------------------ 1) Load & basic filtering ------------------\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "# Keep your filter if you want only good-ish runs; for STeP-style you can choose to relax this.\n",
    "df = df[df[\"success\"] == 1.0]\n",
    "df = df[df[\"total_reward\"] >= 1].reset_index(drop=True)\n",
    "\n",
    "# ------------------ 2) Helpers ------------------\n",
    "def _json_or_ast_load(s: str):\n",
    "    if isinstance(s, (list, dict)): return s\n",
    "    s = s.strip()\n",
    "    try: return json.loads(s)\n",
    "    except Exception: return ast.literal_eval(s)\n",
    "\n",
    "def strip_fewshot_from_system(sys_text: str) -> str:\n",
    "    if not sys_text: return sys_text\n",
    "    if TASK_SENTINEL in sys_text:\n",
    "        head = sys_text.split(\"\\n\", 1)[0]\n",
    "        tail = sys_text.split(TASK_SENTINEL, 1)[1]\n",
    "        return (head + \"\\n\\n\" + TASK_SENTINEL + tail).strip()\n",
    "    return sys_text.strip()\n",
    "\n",
    "# Simple heuristic for “incorrect” assistant steps (customize as you like)\n",
    "def is_incorrect_assistant(content: str) -> bool:\n",
    "    if not content: return True\n",
    "    if \"Invalid action!\" in content: return True\n",
    "    if \"click[Back to Search]\" in content: return True\n",
    "    return False\n",
    "\n",
    "# ------------------ 3) Tokenizer (Qwen chat) ------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "try: tokenizer.truncation_side = \"left\"\n",
    "except: pass\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# ------------------ 4) Build ONE SAMPLE PER TRAJECTORY ------------------\n",
    "# Emits:\n",
    "#   text            -> full episode with \"<|im_start|>role ... <|im_end|>\"\n",
    "#   assistant_spans -> list of {\"start_char\",\"end_char\",\"correct\":bool,\"idx\":int}\n",
    "#\n",
    "# NOTE: we *manually* compose Qwen-style blocks so we can track exact char offsets.\n",
    "# This matches Qwen’s standard chat markers and works well for span bookkeeping.\n",
    "def build_full_episode_with_spans(trace):\n",
    "    # 1) Normalize + optional system trimming\n",
    "    sys_msg = next((m.get(\"content\",\"\") for m in trace if m.get(\"role\")==\"system\"), \"\")\n",
    "    sys_msg = strip_fewshot_from_system(sys_msg)\n",
    "\n",
    "    # 2) Rebuild the full message list in order, including system first (if present)\n",
    "    msgs = []\n",
    "    if sys_msg:\n",
    "        msgs.append({\"role\": \"system\", \"content\": sys_msg})\n",
    "    # keep original order of user/assistant turns\n",
    "    for m in trace:\n",
    "        role = m.get(\"role\")\n",
    "        if role not in (\"user\", \"assistant\"): \n",
    "            continue\n",
    "        content = (m.get(\"content\") or \"\").strip()\n",
    "        msgs.append({\"role\": role, \"content\": content})\n",
    "\n",
    "    # 3) Serialize with Qwen markers and record assistant spans\n",
    "    text_parts = []\n",
    "    assistant_spans = []\n",
    "    cursor = 0\n",
    "    assistant_idx = 0\n",
    "\n",
    "    for m in msgs:\n",
    "        role = m[\"role\"]\n",
    "        content = m[\"content\"]\n",
    "\n",
    "        # Qwen-style block\n",
    "        prefix = f\"<|im_start|>{role}\\n\"\n",
    "        body   = content\n",
    "        suffix = \"<|im_end|>\\n\"\n",
    "\n",
    "        block = prefix + body + suffix\n",
    "        # span bookkeeping for assistant\n",
    "        if role == \"assistant\":\n",
    "            start_char = cursor + len(prefix)\n",
    "            end_char   = start_char + len(body)\n",
    "            assistant_spans.append({\n",
    "                \"start_char\": start_char,\n",
    "                \"end_char\":   end_char,\n",
    "                \"correct\":    (not is_incorrect_assistant(content)),\n",
    "                \"idx\":        assistant_idx,   # 0-based assistant turn index\n",
    "            })\n",
    "            assistant_idx += 1\n",
    "\n",
    "        text_parts.append(block)\n",
    "        cursor += len(block)\n",
    "\n",
    "    full_text = \"\".join(text_parts)\n",
    "\n",
    "    # (Optional) Add a final generation cue if you’d like the model to continue after the last block:\n",
    "    # full_text += \"<|im_start|>assistant\\n\"   # <-- only if you want to *generate* one more step\n",
    "\n",
    "    return full_text, assistant_spans\n",
    "\n",
    "def row_to_full_episode(row):\n",
    "    trace = _json_or_ast_load(row.get(\"orig_trace\", \"[]\"))\n",
    "    if not isinstance(trace, list) or not trace:\n",
    "        return None, None\n",
    "    text, spans = build_full_episode_with_spans(trace)\n",
    "    return text, spans\n",
    "\n",
    "# Build dataset rows\n",
    "records = []\n",
    "for _, row in df.iterrows():\n",
    "    text, spans = row_to_full_episode(row)\n",
    "    if not text: \n",
    "        continue\n",
    "    # Keep ALL steps (including incorrect) — do NOT filter them out.\n",
    "    # You’ll decide masking in your collator later.\n",
    "    records.append({\n",
    "        \"text\": text,\n",
    "        \"assistant_spans\": json.dumps(spans, ensure_ascii=False),\n",
    "        # (Optional) carry episode-level metadata you might want later:\n",
    "        # \"category\": row.get(\"category\", None),\n",
    "        # \"total_reward\": float(row.get(\"total_reward\", 0)),\n",
    "        # \"success\": int(row.get(\"success\", 0)),\n",
    "    })\n",
    "\n",
    "df_out = pd.DataFrame.from_records(records)\n",
    "print(f\"Built trajectories: {len(df_out)}\")\n",
    "\n",
    "# ------------------ 5) Optional handpicked top-20 per category ------------------\n",
    "if USE_HANDPICKED:\n",
    "    raise NotImplementedError(\"Apply handpicked filtering BEFORE constructing full trajectories.\")\n",
    "\n",
    "# ------------------ 6) HF dataset ------------------\n",
    "# One item per trajectory, with full text + per-assistant spans\n",
    "dataset = Dataset.from_pandas(df_out[[\"text\", \"assistant_spans\"]].copy())\n",
    "hf_ds = dataset\n",
    "\n",
    "# Quick peek\n",
    "print(hf_ds[0][\"text\"][:500])\n",
    "print(hf_ds[0][\"assistant_spans\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9c40b23b-be1d-4fce-8db1-409eded51980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'completion'],\n",
       "    num_rows: 403\n",
       "})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "894c3061-8e53-4185-a780-8c29c23f736f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**************************************************\n",
      "Total removed examples (with Invalid Action!): 40\n",
      "**************************************************\n",
      "403\n",
      "Average Reward and Count by category:\n",
      "             reward_average  count\n",
      "category                          \n",
      "beauty                  1.0    102\n",
      "electronics             1.0     70\n",
      "fashion                 1.0     87\n",
      "food                    1.0     69\n",
      "furniture               1.0     75\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/train_set_difficulty_scoring.csv\") #VANILLA DATASET\n",
    "# df = pd.read_csv(\"data/react_train_set_difficulty_scoring.csv\") #VANILLA DATASET\n",
    "# df = pd.read_csv(\"data/rag_train_set_difficulty_scoring.csv\") #RAG DATASET\n",
    "\n",
    "use_handpicked = False\n",
    "# use_handpicked = True\n",
    "\n",
    "df = df[df.success==1.0]\n",
    "df = df[df.total_reward>=1.0]\n",
    "# df = df[df.num_of_steps<=9]\n",
    "# df = df[df.num_of_steps>5]\n",
    "\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "def calc_successes(df):\n",
    "    # Group by env_type\n",
    "    grouped = df.groupby(\"category\")[\"total_reward\"]\n",
    "    \n",
    "    # Compute success rate and count\n",
    "    success_stats = grouped.agg(\n",
    "        reward_average=lambda x: x.mean(),\n",
    "        count=\"count\"\n",
    "    ).round(2)\n",
    "    \n",
    "    # Print\n",
    "    print(\"Average Reward and Count by category:\")\n",
    "    print(success_stats.to_string())\n",
    "\n",
    "\n",
    "#  Invalid action!\n",
    "def extract_whole_trace(example):\n",
    "    trace = ast.literal_eval(example.orig_trace)\n",
    "\n",
    "    out_lines = []\n",
    "    for m in trace:\n",
    "        role = m[\"role\"]\n",
    "        content = m[\"content\"].strip()\n",
    "        if role == \"system\":\n",
    "            out_lines.append(f\"system: {content}\")\n",
    "        elif role == \"user\":\n",
    "            out_lines.append(f\"user: {content}\")\n",
    "        elif role == \"assistant\":\n",
    "            out_lines.append(f\"assistant: {content}\")\n",
    "        else:\n",
    "            out_lines.append(f\"{role}: {content}\")  # fallback\n",
    "\n",
    "    # First line(s) = prompt\n",
    "    sys_msg = next(m[\"content\"] for m in trace if m[\"role\"]==\"system\")\n",
    "    usr_msg = next(m[\"content\"] for m in trace if m[\"role\"]==\"user\")\n",
    "    prompt  = f\"{sys_msg.strip()}\\n{usr_msg.strip()}\\n\"\n",
    "\n",
    "    # The rest is the completion (assistant + further user turns)\n",
    "    rest     = trace[2:]\n",
    "    completion = \"\".join(\n",
    "        (f\"{m['role']}: {m['content'].strip()}\\n\") for m in rest\n",
    "        # (f\"{m['content'].strip()}\\n\") for m in rest\n",
    "    )\n",
    "    # completion = \"\".join(m[\"content\"] + \"\\n\" for m in rest)\n",
    "\n",
    "    if not completion.endswith(\"\"):\n",
    "        completion += \"\"\n",
    "\n",
    "    match = re.findall(\"Invalid action!\", completion)\n",
    "    # print(match)\n",
    "    \n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        \"completion\": completion,\n",
    "        \"nothing_occ\": len(match)\n",
    "    }\n",
    "    \n",
    "# Apply to your dataframe\n",
    "df[[\"prompt\", \"completion\", \"nothing_occ\"]] = df.apply(\n",
    "    lambda row: pd.Series(extract_whole_trace(row)), axis=1\n",
    ")\n",
    "\n",
    "removed_examples = len(df[df.nothing_occ != 0])\n",
    "df = df[df.nothing_occ == 0]\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"\\n\"+\"*\"*50 + f\"\\nTotal removed examples (with Invalid Action!): {removed_examples}\\n\" + \"*\"*50)\n",
    "\n",
    "if use_handpicked:\n",
    "    # df = df[df.success==1].copy()\n",
    "    df = df[df.total_reward >= 1.0]\n",
    "    df['expert_difficulty'] = pd.to_numeric(df['expert_difficulty'], errors='coerce')\n",
    "    df = df.dropna(subset=['expert_difficulty'])\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    df = (\n",
    "        df.sort_values(['category', 'expert_difficulty'], ascending=[True, False])\n",
    "          .groupby('category', as_index=False, group_keys=False)\n",
    "          .head(20)\n",
    "    )\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "print(len(df))\n",
    "# df[[\"prompt\", \"completion\", \"nothing_occ\"]] = df.apply(lambda row: pd.Series(extract_whole_trace(row)), axis=1)\n",
    "\n",
    "calc_successes(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3bcc8890-1a95-4fef-8e67-8fa2dc5a6e04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are a shopping assistant. Buy the closest item to the instruction provided. \\nClick only from the available buttons/options shown in square brackets [].\\nYou can use search action if search is available. Remember to click[Back to Search] to go back to the search menu.\\nAlways ensure the click value exactly matches one of the listed options. \\n\\n\\n============\\n\\nHere is 1 example:\\n\\nInstruction:  \\ni would like a 3 ounce bottle of bright citrus deodorant for sensitive skin, and price lower than 50.00 dollars \\n[Search]  \\n</turn>\\n<turn>\\nGoal: Buy a 3 ounce bottle of bright citrus deodorant for sensitive skin, and price lower than 50.00 dollars\\nCurrent Location: Search Home Page\\nCurrent Selection: None\\nAction: search[3 ounce bright citrus deodorant sensitive skin]\\n</turn>\\n<turn>\\nObservation: \\n[Back to Search] \\nPage 1 (Total results: 50) \\n[Next >] \\n[B078GWRC1J] \\nBright Citrus Deodorant by Earth Mama | Natural and Safe for Sensitive Skin, Pregnancy and Breastfeeding, Contains Organic Calendula 3-Ounce \\n$10.99 \\n[B078GTKVXY] \\nGinger Fresh Deodorant by Earth Mama | Natural and Safe for Sensitive Skin, Pregnancy and Breastfeeding, Contains Organic Calendula 3-Ounce \\n$10.99 \\n[B08KBVJ4XN] \\nBarrel and Oak - Aluminum-Free Deodorant, Deodorant for Men, Essential Oil-Based Scent, 24-Hour Odor Protection, Cedar & Patchouli Blend, Gentle on Sensitive Skin (Mountain Sage, 2.7 oz, 2-Pack) \\n$15.95  \\n</turn>\\n<turn>\\nGoal: Buy a 3 ounce bottle of bright citrus deodorant for sensitive skin, and price lower than 50.00 dollars\\nCurrent Location: Search Results Page, page 1\\nCurrent Selection: None\\nAction: click[B078GWRC1J]\\n</turn>\\n<turn>\\nObservation: \\n[Back to Search] \\n[< Prev] \\nscent [assorted scents][bright citrus][calming lavender][ginger fresh][simply non-scents]\\nsize [travel set (4-pack)][3 ounce (pack of 1)][3-ounce (2-pack)]\\nBright Citrus Deodorant by Earth Mama | Natural and Safe for Sensitive Skin, Pregnancy and Breastfeeding, Contains Organic Calendula 3-Ounce \\nPrice: $10.99 \\nRating: N.A. \\n[Description] \\n[Features] \\n[Reviews] \\n[Buy Now]  \\n</turn>\\n<turn>\\nGoal: Buy a 3 ounce bottle of bright citrus deodorant for sensitive skin, and price lower than 50.00 dollars\\nCurrent Location: Item B078GWRC1J Page\\nCurrent Selection: None\\nAction: click[bright citrus]\\n</turn>\\n<turn>\\nObservation: You have clicked bright citrus. \\n</turn>\\n<turn>\\nGoal: Buy a 3 ounce bottle of bright citrus deodorant for sensitive skin, and price lower than 50.00 dollars\\nCurrent Location: Item B078GWRC1J Page\\nCurrent Selection: bright citrus\\nAction: click[3 ounce (pack of 1)]\\n</turn>\\n<turn>\\nObservation: You have clicked 3 ounce (pack of 1). \\n</turn>\\n<turn>\\nGoal: Buy a 3 ounce bottle of bright citrus deodorant for sensitive skin, and price lower than 50.00 dollars\\nCurrent Location: Item B078GWRC1J Page\\nCurrent Selection: bright citrus, 3 ounce (pack of 1)\\nAction: click[Buy Now]\\n</turn>\\n\\n============\\n\\nHere is the task.\\nWebShop \\nInstruction:  \\ni need a quick drying running shorts with drawstring closure. it should be light grayish blue in color, and price lower than 60.00 dollars \\n[Search]\\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.prompt[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "83c044eb-3354-42d2-a782-d30d400c5dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to entire DataFrame\n",
    "df[\"prompt\"] = df[\"prompt\"].apply(remove_few_shot_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c71f40e1-d674-4bbc-8d76-99b13f350382",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'prompt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[32m/tmp/ipykernel_19401/4248255214.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df.prompt[\u001b[32m0\u001b[39m]\n",
      "\u001b[32m/workspace/workspace/miniforge3/envs/finetune/lib/python3.11/site-packages/pandas/core/generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   6314\u001b[39m             \u001b[38;5;28;01mand\u001b[39;00m name \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m self._accessors\n\u001b[32m   6315\u001b[39m             \u001b[38;5;28;01mand\u001b[39;00m self._info_axis._can_hold_identifiers_and_holds_name(name)\n\u001b[32m   6316\u001b[39m         ):\n\u001b[32m   6317\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self[name]\n\u001b[32m-> \u001b[39m\u001b[32m6318\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m object.__getattribute__(self, name)\n",
      "\u001b[31mAttributeError\u001b[39m: 'DataFrame' object has no attribute 'prompt'"
     ]
    }
   ],
   "source": [
    "df.prompt[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "90fef2e1-24a6-488a-83d2-0d489e31d82f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['trace_file', 'num_of_steps', 'done', 'error', 'early_stop', 'total_prompt_token', 'total_in_token_accumulated', 'total_in_token_message_accumulated', 'total_out_token_accumulated', 'total_history_token', 'total_hint_prompt_token', 'total_hint_completion_token', 'expert_difficulty', 'category', 'expert_traces', 'orig_trace', 'prompt', 'completion', 'nothing_occ'],\n",
       "    num_rows: 403\n",
       "})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset = Dataset.from_pandas(df)\n",
    "dataset = Dataset.from_pandas(df.drop(columns=[\"success\", \"total_reward\"]))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4e299511-173b-4ee2-ae4e-74d931bd60df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"user: WebShop \\nInstruction:  \\ni need a quick drying running shorts with drawstring closure. it should be light grayish blue in color, and price lower than 60.00 dollars \\n[Search]\\nassistant: <turn>\\nGoal: Buy quick drying running shorts with drawstring closure, light grayish blue in color, and price lower than 60.00 dollars\\nCurrent Location: Search Home Page\\nCurrent Selection: None\\nAction: search[quick drying running shorts light grayish blue]\\nuser: [Back to Search] \\nPage 1 (Total results: 50) \\n[Next >] \\n[B08DKM7Y6G] \\nCRZ YOGA Women's Mid-Rise Quick Dry Workout Running Shorts Loose Drawstring Athletic Gym Shorts with Zip Pocket -3 Inches \\n$28.0 \\n[B08DR3PS84] \\nCRZ YOGA Women Mid-Rise Workout Running Shorts with Liner 2 in 1 Athletic Sports Shorts with Zip Pocket- 3 inches \\n$28.0 \\n[B07D6ZRG7B] \\nCRZ YOGA Quick-Dry Loose Running Shorts Mid Waist Sports Workout Shorts for Women Gym Athletic Shorts with Pocket - 2.5'' \\n$28.0\\nassistant: <turn>\\nGoal: Buy quick drying running shorts with drawstring closure, light grayish blue in color, and price lower than 60.00 dollars\\nCurrent Location: Search Results Page, page 1\\nCurrent Selection: None\\nAction: click[B08DKM7Y6G]\\nuser: [Back to Search] \\n[< Prev] \\nsize [xx-small][x-small][small][medium][large][x-large]\\ncolor [black][brown rock][camo multi 1][coconut milk white][graphite grey][light grayish blue][mineral brown][stelindigo][the dark purple][dark grey camouflage][true navy][mountain olive]\\nCRZ YOGA Women's Mid-Rise Quick Dry Workout Running Shorts Loose Drawstring Athletic Gym Shorts with Zip Pocket -3 Inches \\nPrice: $28.0 \\nRating: N.A. \\n[Description] \\n[Features] \\n[Reviews] \\n[Buy Now]\\nassistant: <turn>\\nGoal: Buy quick drying running shorts with drawstring closure, light grayish blue in color, and price lower than 60.00 dollars\\nCurrent Location: Item B08DKM7Y6G Page\\nCurrent Selection: None\\nAction: click[light grayish blue]\\nuser: You have clicked light grayish blue.\\nassistant: <turn>\\nGoal: Buy quick drying running shorts with drawstring closure, light grayish blue in color, and price lower than 60.00 dollars\\nCurrent Location: Item B08DKM7Y6G Page\\nCurrent Selection: light grayish blue\\nAction: click[medium]\\nuser: You have clicked medium.\\nassistant: <turn>\\nGoal: Buy quick drying running shorts with drawstring closure, light grayish blue in color, and price lower than 60.00 dollars\\nCurrent Location: Item B08DKM7Y6G Page\\nCurrent Selection: light grayish blue, medium\\nAction: click[Buy Now]\\nuser: Your score (min 0.0, max 1.0): 1.0\\n\""
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.completion[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "49c6b759-4305-488f-a6d5-7c560ad83bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Steps Statistics\n",
      "***************************\n",
      "Min 4\n",
      "25th 4.0\n",
      "Median 5.0\n",
      "Mean 5.142493638676845\n",
      "75th 6.0\n",
      "Max 9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Steps Statistics\\n***************************\")\n",
    "print(f\"\"\"Min {df.num_of_steps.min()}\n",
    "25th {df.num_of_steps.quantile(.25)}\n",
    "Median {df.num_of_steps.median()}\n",
    "Mean {df.num_of_steps.mean()}\n",
    "75th {df.num_of_steps.quantile(.75)}\n",
    "Max {df.num_of_steps.max()}\n",
    "\"\"\")\n",
    "# df.num_of_steps.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7a019833-7b07-42a1-8f03-5636eee4f966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. (Optional) Split into train/validation\n",
    "# Here we do an 90/10 split; adjust 'seed' for reproducibility\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "train_ds = dataset[\"train\"]\n",
    "val_ds   = dataset[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1418569-34f4-40df-9a82-c284ba1ce776",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'completion'],\n",
       "    num_rows: 1937\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db05302f-2816-4ee1-bc9c-bc01b24fb336",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25513/1722145986.py:1: UserWarning: WARNING: Unsloth should be imported before transformers to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth import FastLanguageModel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 08-31 22:09:22 [__init__.py:235] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from unsloth.chat_templates import train_on_responses_only\n",
    "\n",
    "def get_model_and_tokenizer(\n",
    "        model_name=\"Qwen/Qwen2.5-14B-Instruct\", \n",
    "        max_seq_length=1024,\n",
    "        load_in_4bit=True, \n",
    "        gpu_memory_utilization=0.99,\n",
    "        lora_rank=16,\n",
    "        random_state=1234,\n",
    "        get_peft=True,\n",
    "        # lora_alpha=64\n",
    "    ):\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = model_name ,\n",
    "        max_seq_length = max_seq_length,\n",
    "        load_in_4bit = load_in_4bit, # False for LoRA 16bit\n",
    "        load_in_8bit = False,    # A bit more accurate, uses 2x memory\n",
    "        full_finetuning = False, # We have full finetuning now!\n",
    "        fast_inference = False, # Enable vLLM fast inference\n",
    "        max_lora_rank = lora_rank,\n",
    "        gpu_memory_utilization = gpu_memory_utilization, # Reduce if out of memory\n",
    "        # device_map=\"cuda\" ,               # or {\"\": 0} to pin to GPU 0\n",
    "        device_map=\"auto\",               # pin everything to GPU 0\n",
    "        # trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    if get_peft:\n",
    "        model = FastLanguageModel.get_peft_model(\n",
    "            model,\n",
    "            r = lora_rank, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "            target_modules = [\n",
    "                \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "            ], # Remove QKVO if out of memory\n",
    "            # lora_alpha = lora_rank,\n",
    "            lora_alpha = lora_rank*2,\n",
    "            \n",
    "            lora_dropout = 0.2,                    # add dropout (try 0.1–0.2)\n",
    "            \n",
    "            use_gradient_checkpointing = \"unsloth\", # Enable long context finetuning\n",
    "            random_state = random_state,\n",
    "        )\n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18c6c126-3fee-48db-b502-2d349a3f0768",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "def get_sft_trainer(model, tokenizer, dataset, eval_dataset=None, training_steps=10, collator=None, type_field=\"text\"):\n",
    "    training_args = SFTConfig(\n",
    "        dataset_text_field = type_field,\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4, # Use GA to mimic batch size!\n",
    "        # warmup_steps = 5,\n",
    "        # warmup_steps = 10,\n",
    "        warmup_ratio = 0.1,\n",
    "        \n",
    "        num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        # max_steps = training_steps,\n",
    "\n",
    "        label_smoothing_factor=0.1,\n",
    "        learning_rate = 2e-4, # Reduce to 2e-5 for long training runs\n",
    "        # learning_rate = 2e-5, # Reduce to 2e-5 for long training runs\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        \n",
    "        # weight_decay = 0.01,\n",
    "        weight_decay = 0.05, #increase to maybe improve\n",
    "        \n",
    "        lr_scheduler_type = \"linear\",\n",
    "        # lr_scheduler_type = \"cosine\",\n",
    "        seed = 42,\n",
    "        \n",
    "        # eval_strategy=\"steps\",\n",
    "        # eval_steps=3,                      # <-- frequency\n",
    "        \n",
    "        # report_to = \"none\", # Use this for WandB etc\n",
    "    )\n",
    "    \n",
    "    trainer = SFTTrainer(\n",
    "        model = model,\n",
    "        tokenizer = tokenizer,\n",
    "        train_dataset = dataset,\n",
    "        eval_dataset = eval_dataset, # Can set up evaluation!\n",
    "        args = training_args,\n",
    "        data_collator = collator,\n",
    "    )\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88895407-4f2f-4bd4-9d67-9edc44ce417b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "294"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "try:\n",
    "    del model\n",
    "    del tokenizer\n",
    "except:\n",
    "    pass\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39597fc1-42ae-4ca4-85da-4d9a8d8ee935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.8.1: Fast Qwen2 patching. Transformers: 4.55.0. vLLM: 0.10.0.\n",
      "   \\\\   /|    NVIDIA A100-SXM4-80GB. Num GPUs = 1. Max memory: 79.254 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 8.0. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.18it/s]\n",
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.2.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
      "Unsloth 2025.8.1 patched 28 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "# model, tokenizer = get_model_and_tokenizer(max_seq_length=16000)\n",
    "# model, tokenizer = get_model_and_tokenizer()\n",
    "model, tokenizer = get_model_and_tokenizer(model_name=\"Qwen/Qwen2.5-7B-Instruct\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91be8bfb-88d8-4996-80e1-cbcd3694acf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 380/380 [00:00<00:00, 27867.18 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Prepare HF datasets\n",
    "def merge_prompt_completion(example):\n",
    "    # Make sure your completion already ends with EOS\n",
    "    return {\"text\": example[\"prompt\"] + example[\"completion\"]}\n",
    "\n",
    "try:\n",
    "    hf_ds = dataset.map(\n",
    "    merge_prompt_completion,\n",
    "    remove_columns=[c for c in dataset[\"train\"].column_names if c not in (\"text\",)],\n",
    "    )\n",
    "except:\n",
    "    hf_ds = dataset.map(\n",
    "        merge_prompt_completion,\n",
    "        remove_columns=[c for c in dataset.column_names if c not in (\"text\",)],\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "collator = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34cedc9f-8209-4be1-8807-df742bc92e4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'completion'],\n",
       "    num_rows: 341\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3997a54c-4e66-44e0-bd55-3b2563a6ef2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# hf_ds['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3ab6452-df7c-4b7b-bd53-f52f10995618",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|im_start|>system\\nYou are a shopping assistant. Buy the closest item to the instruction provided. \\n\\nHere is the task.<|im_end|>\\n<|im_start|>user\\nWebShop \\nInstruction:  \\ni am interested in a high quality brush set, and price lower than 30.00 dollars \\n[Search]<|im_end|>\\n<|im_start|>assistant\\nassistant:  <turn>\\nAction: search[high quality brush set < $30]\\nuser: \\n[Back to Search] \\nPage 1 (Total results: 50) \\n[Next >] \\n[B07HMRCPFQ] \\nClearance! Lmtime High-End 18 pcs Makeup Brush Set tools Make-up Toiletry Kit Wool Make Up Brush Set (Purple) \\n$2.99 \\n[B07PWW6PKQ] \\nBEILI Makeup Brushes 30Pcs Professional Makeup Brush Set Premium Pink Vegan Synthetic Kabuki Foundation Blending Brush Face Powder Blush Concealers Eye Shadows Make Up Brush Set (White/Rose Gold) \\n$42.99 \\n[B01G6AUR32] \\nvela.yue Precise Crease Brush Eyes Tapered Blending Contour Makeup Tool \\n$5.99\\nassistant:  \\n<turn>\\nAction: think[The only brush set under 30 dollars is B07HMRCPFQ. Let's check it out.]\\nuser: \\n[Back to Search] \\nPage 1 (Total results: 50) \\n[Next >] \\n[B07HMRCPFQ] \\nClearance! Lmtime High-End 18 pcs Makeup Brush Set tools Make-up Toiletry Kit Wool Make Up Brush Set (Purple) \\n$2.99 \\n[B07PWW6PKQ] \\nBEILI Makeup Brushes 30Pcs Professional Makeup Brush Set Premium Pink Vegan Synthetic Kabuki Foundation Blending Brush Face Powder Blush Concealers Eye Shadows Make Up Brush Set (White/Rose Gold) \\n$42.99 \\n[B01G6AUR32] \\nvela.yue Precise Crease Brush Eyes Tapered Blending Contour Makeup Tool \\n$5.99\\nassistant:  \\n\\n<turn>\\nAction: click[B07HMRCPFQ]\\nuser: \\n[Back to Search] \\n[< Prev] \\ncolor [black][pink][purple]\\nClearance! Lmtime High-End 18 pcs Makeup Brush Set tools Make-up Toiletry Kit Wool Make Up Brush Set (Purple) \\nPrice: $2.99 \\nRating: N.A. \\n[Description] \\n[Features] \\n[Reviews] \\n[Buy Now]\\nassistant:  \\n\\n<turn>\\nAction: think[The color options are black, pink, and purple. Since the default color is purple, I'll select that.]\\nuser: \\n[Back to Search] \\n[< Prev] \\ncolor [black][pink][purple]\\nClearance! Lmtime High-End 18 pcs Makeup Brush Set tools Make-up Toiletry Kit Wool Make Up Brush Set (Purple) \\nPrice: $2.99 \\nRating: N.A. \\n[Description] \\n[Features] \\n[Reviews] \\n[Buy Now]\\nassistant:  \\n\\n<turn>\\nAction: click[purple]\\nuser: You have clicked purple.\\nassistant:  \\n\\n<turn>\\nAction: think[The color is now set to purple. Now I can proceed to buy the item.]\\nuser: \\n[Back to Search] \\n[< Prev] \\ncolor [black][pink][[purple]]\\nClearance! Lmtime High-End 18 pcs Makeup Brush Set tools Make-up Toiletry Kit Wool Make Up Brush Set (Purple) \\nPrice: $2.99 \\nRating: N.A. \\n[Description] \\n[Features] \\n[Reviews] \\n[Buy Now]\\nassistant:  \\n\\n<turn>\\nAction: click[Buy Now]\\nuser: Your score (min 0.0, max 1.0): 1.0\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_ds[\"text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a20c52d-22c5-426a-a0b3-e4d96feb9511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "================\\TRAINING SFT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Tokenizing [\"text\"] (num_proc=2): 100%|██████████| 341/341 [00:00<00:00, 351.49 examples/s]\n",
      "Map (num_proc=255): 100%|██████████| 341/341 [00:03<00:00, 86.23 examples/s] \n"
     ]
    }
   ],
   "source": [
    "from unsloth.chat_templates import train_on_responses_only\n",
    "\n",
    "# hf_ds = dataset\n",
    "\n",
    "print(\"\\n\\n================\\TRAINING SFT\")\n",
    "\n",
    "collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True)\n",
    "\n",
    "# sft_trainer = get_sft_trainer(model, tokenizer, hf_ds, training_steps=15, collator=collator, type_field=\"text\")\n",
    "sft_trainer = get_sft_trainer(model, tokenizer, hf_ds, training_steps=15, collator=collator, type_field=\"text\")\n",
    "\n",
    "sft_trainer = train_on_responses_only(\n",
    "    sft_trainer,\n",
    "    instruction_part = \"<|im_start|>user\",\n",
    "    response_part    = \"<|im_start|>assistant\",\n",
    "    force_match      = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7863190c-d14c-4152-9a0a-80d055733fed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27582426-dad6-4a72-b2ce-3755a849c274",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "================\\TRAINING SFT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Tokenizing [\"text\"] (num_proc=2): 100%|██████████| 380/380 [00:01<00:00, 374.19 examples/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n================\\TRAINING SFT\")\n",
    "\n",
    "try:\n",
    "    sft_trainer = get_sft_trainer(model, tokenizer, hf_ds[\"train\"], eval_dataset=hf_ds[\"test\"], training_steps=15, collator=collator)\n",
    "except:\n",
    "    sft_trainer = get_sft_trainer(model, tokenizer, hf_ds, training_steps=15, collator=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08206a2a-a326-47d9-9f45-50026c6da4c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 380 | Num Epochs = 1 | Total steps = 48\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 40,370,176 of 7,655,986,688 (0.53% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='48' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [48/48 01:54, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>19.597100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>18.933500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>18.950900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>18.346200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>16.322100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>14.829000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>14.059100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>13.407700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>13.326800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>12.811400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>12.405900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>12.007400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>11.952200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>11.502200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>11.147300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>10.755100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>10.019800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>9.876400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>10.011500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>9.413200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>9.963400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>9.561300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>9.429000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>9.855200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>9.565600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>9.557200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>9.420500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>9.566500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>9.713800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>9.079900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>9.303700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>9.266800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>9.392000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>9.028800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>9.295700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>9.047300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>9.369000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>9.416100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>9.294100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>9.344400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>8.783900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>9.009100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>8.976200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>9.345600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>9.244900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>9.008800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>8.941300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>4.503200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "sft_trainer.train()\n",
    "print(\"\\n\\n\\n\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "370de55e-efb2-4a45-8f8e-a27722cda05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wherever you want to save:\n",
    "\n",
    "# out_dir = \"models/14B_vanilla_all\"\n",
    "# out_dir = \"models/14B_vanilla_20handpicked\"\n",
    "\n",
    "# out_dir = \"models/14B_rag_all\"\n",
    "# out_dir = \"models/14B_rag_20handpicked\"\n",
    "\n",
    "# out_dir = \"models/7B_vanilla_all\"\n",
    "# out_dir = \"models/7B_vanilla_20handpicked\"\n",
    "\n",
    "# out_dir = \"models/7B_rag_all\"\n",
    "# out_dir = \"models/7B_rag_20handpicked\"\n",
    "\n",
    "\n",
    "# out_dir = \"models/14B_vanilla_all\"\n",
    "# out_dir = \"models/14B_vanilla_20handpicked\"\n",
    "\n",
    "# out_dir = \"models/14B_vanilla_all_smoothing\"\n",
    "\n",
    "\n",
    "\n",
    "# out_dir = \"models/14B_state_rag_self_smoothing\"\n",
    "# out_dir = \"models/14B_react_rag_self_smoothing\"\n",
    "\n",
    "\n",
    "\n",
    "# out_dir = \"models/7B_react_self_smoothing\"\n",
    "# out_dir = \"models/7B_react_rag_self_smoothing\"\n",
    "\n",
    "out_dir = \"models/7B_state_rag_self_smoothing\"\n",
    "\n",
    "\n",
    "# write model + tokenizer\n",
    "model.save_pretrained(out_dir)\n",
    "tokenizer.save_pretrained(out_dir)\n",
    "sft_trainer.save_model(out_dir)               # saves model, config, trainer state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "434da7f7-4369-4613-bd35-50d5bbed1d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_lora(\"qwen3b-sft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42fb9f0-4be7-4f38-bb9b-66e464adc98c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7a1472127254f2a8d3cae07af144a22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-14B-Instruct\", trust_remote_code=True)\n",
    "tokenizer  = AutoTokenizer.from_pretrained(out_dir, trust_remote_code=True)\n",
    "\n",
    "# adapter_config is picked up automatically from output_dir\n",
    "model = PeftModel.from_pretrained(base_model, out_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a32c33b1-0453-4316-9efa-5f87be860d77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0380eb987d384daa852471481047efe4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "/rds/general/user/hai24/home/miniforge3/envs/fine-tune/lib/python3.11/site-packages/accelerate/utils/modeling.py:1569: UserWarning: Current model requires 256 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded OK ✓\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path, PurePath\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "lora_path = Path(\"/rds/general/user/hai24/home/StateAct/alfworld_runs/models/14B_full_hard-abovemean_nofewshot_fixed\")\n",
    "\n",
    "# lora_path = Path(\"/rds/general/user/hai24/home/StateAct/alfworld_runs/models/14B_full_15steps_nofewshot_fixed_sanity\")\n",
    "\n",
    "assert lora_path.exists(), f\"{lora_path} missing!\"\n",
    "\n",
    "base_id = \"Qwen/Qwen2.5-14B-Instruct\"     # ← swap to the correct base\n",
    "model = AutoModelForCausalLM.from_pretrained(base_id,\n",
    "                                             device_map=\"auto\",\n",
    "                                             torch_dtype=\"auto\")\n",
    "model = PeftModel.from_pretrained(model, lora_path, torch_dtype=\"auto\")\n",
    "print(\"Loaded OK ✓\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6041d15-3a52-45c3-978d-a846e27e0364",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (finetune)",
   "language": "python",
   "name": "finetune"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
